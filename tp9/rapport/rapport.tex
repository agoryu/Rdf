  % --------------------------------------
% Document Class
% --------------------------------------
\documentclass[a4paper,11pt]{article}
% --------------------------------------



% --------------------------------------
% Use Package
% --------------------------------------


\usepackage[francais]{babel}
%\usepackage{ucs}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{makeidx}
\usepackage{color}
\usepackage{graphicx}
\usepackage{float}
\usepackage[hidelinks]{hyperref} 
\usepackage{geometry}
%\usepackage{lastpage}
%\usepackage{marginnote}
\usepackage{fancyhdr}
%\usepackage{titlesec}
%\usepackage{framed}
\usepackage{amsmath}
\usepackage{empheq}
\usepackage{array}
\usepackage{multicol}
\usepackage{csquotes}
%\usepackage{adjustbox}

% insert code
\usepackage{listings}

% define our color
\usepackage{xcolor}

% code color
\definecolor{ligthyellow}{RGB}{250,247,220}
\definecolor{darkblue}{RGB}{5,10,85}
\definecolor{ligthblue}{RGB}{1,147,128}
\definecolor{darkgreen}{RGB}{8,120,51}
\definecolor{darkred}{RGB}{160,0,0}

% other color
\definecolor{ivi}{RGB}{141,107,185}


\lstset{
  language=R,
  captionpos=b,
  extendedchars=true,
  frame=lines,
  numbers=left,
  numberstyle=\tiny,
  numbersep=5pt,
  keepspaces=true,
  breaklines=true,
  showspaces=false,
  showstringspaces=false,
  breakatwhitespace=false,
  stepnumber=1,
  showtabs=false,
  tabsize=3,
  basicstyle=\small\ttfamily,
  backgroundcolor=\color{ligthyellow},
  keywordstyle=\color{ligthblue},
  morekeywords={include, printf, uchar},
  identifierstyle=\color{darkblue},
  commentstyle=\color{darkgreen},
  stringstyle=\color{darkred},
}


% --------------------------------------



% --------------------------------------
% Page setting
% --------------------------------------
%\pagestyle{empty}
\setlength{\headheight}{15pt}

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{2}

\makeatletter
\@addtoreset{chapter}{part}
\makeatother 

\hypersetup{         % parametrage des hyperliens
  colorlinks=true,      % colorise les liens
  breaklinks=true,      % permet les retours à la ligne pour les liens trop longs
  urlcolor= blue,       % couleur des hyperliens
  linkcolor= black,     % couleur des liens internes aux documents (index, figures, tableaux, equations,...)
  citecolor= green      % couleur des liens vers les references bibliographiques
}

% --------------------------------------

% --------------------------------------
% Information
% --------------------------------------
\title{Compte-rendu TP9 Rdf : Arbres de décision}
\author{Elliot VANEGUE et Gaëtan DEFLANDRE}
% --------------------------------------

\definecolor{myColor}{rgb}{0.5, 0.1, 0.75}

% --------------------------------------
% Begin content
% --------------------------------------
\begin{document}
  
  % Set language to english
  \selectlanguage{francais}
  
  % Start the page counting
  \pagenumbering{arabic}
  
  \maketitle
  
  \mbox{}
  \newpage
  \clearpage
  
  \section*{Introduction}
   Durant ce TP, nous allons voir une nouvelle méthode permettant la séparation des données à partir
   d'un arbre de décision. Le but est de diviser des données en plusieurs étapes et en prenant à chaque fois
   le meilleurs attributs pour diviser le plus efficacement l'ensemble de données que nous avons.

  \section{Question de bon sens}
  Pour la question 1, si le joueur B est sur de pouvoir gagner en quatre propositions alors cela signifie
  que N vaut 23. Imaginons que pour trouver la réponse, B séléctionne à chacune de ces questions la moitié
  de l'ensemble qu'il a pu déterminé précédemment. Nous aurons alors un nombre à trois branches avec les réponses
  \enquote{oui}, \enquote{plus} et \enquote{moins} d'une profondeur de quatre. Nous finirons donc sur un 
  arbre à $2^n$ feuilles où n est le nombre de proposition. Mais il faut également prendre en compte les nombres que B a proposé. Il y a alors
  $\sum_{i=0}^n 2^i$ nombre possible.\\
  
  S'il faut exactement quatre proposition à B pour trouver le nombre séléctionné par A, cela signifie que B peut 
  supprimer tous les nombres qui sépare la racine des feuilles, donc la somme que nous avons déterminé précédemment.
  Il reste alors $2^n$ nombre possible, soit seize proposition à faire.
  
  \section{Jeu du pendu}
  Durant cet exercice, nous allons travailler avec une base de données contenant des noms d'animaux. Il nous
  faudra réaliser un algorithme qui va rechercher le nom d'un animal que l'utilisateur aura choisi.\\
  
  L'algorithme que nous développons utilise un arbre de décision afin d'optimiser la séléction d'attributs qui
  divisera les ensemble de nom. Pour commencer, nous créons un tableau dont chaque indice représente le numéro
  d'une lettre et qui stock le nombre de mot qui utilise une certaine lettre. Cela nous permet de calculer
  la probabilité qu'une lettre soit dans un mot avec le calcul suivant :
  $$ p = h / n $$
  n : nombre de mot\\
  h : tableau construit précédement\\
  p : probabilité qu'une lettre apparaisse dans un mot\\
  
  Cela va nous permettre de calculer l'entropie pour chaque lettre. L'entropie est un nombre qui mesure
  l'incertitude de données. Dans notre cas nous prenons l'entropie le plus élevé. Cela permet de prendre
  le nombre qui va séparer au mieux deux classes. Cette valeur ce mesure avec le calcule suivant :
  $$entropie = -(log2(p ^{p}) + log2(pInv ^{pInv}))$$
  où pInv est la probabilité inverse de p soit $1-p$.\\
  
  Une fois que nous avons l'entropie, nous pouvons créer le jeu du pendu. Le jeu est composé d'une suite de question.
  Une fois que l'utilisateur a choisi un nom dans la liste, l'ordinateur va déterminer ce nom, en minimisant le nombre de questions. Il va demander
  si la lettre qu'il a déterminer est présente dans le mot. L'ordinateur aura au préalable divisé l'ensemble en 
  deux autres ensemble : l'ensemble des noms composés de la lettre trouvé par l'algorithme et l'ensemble des autres mots. 
  Si la lettre est présente dans le mot, l'algorithme continuera sur le premier ensemble, sinon il prendra le second.\\
  
  % A voir
  Lorsque nous regardons la profondeur de l'arbre de décision que nous avons créé pour le jeu, nous voyons que 
  sa prodondeur maximal est de neuf. Cela signifie que l'algorithme peut déterminer un mot en au plus neuf
  questions.
  
  % Mettre tableau des stats
  
  \section{Conclusion}
  Nous avons pu voir durant ce TP que la recherche d'une données parmis un ensemble fini grâce à un arbre de 
  décision peut être très efficace sans demander un grand nombre de calcul.
  
  \section{Annexe}
  \begin{lstlisting}[caption=Ensemble des fonctions du TP]
   getIndice <- function(ens) {
  
  #taille de l'ensemble
  n <- length(ens)
  
  #creation d'une matrice de taille 26 sur nb mot de booleen
  #indique quel lettre sont presentent pour chaque mot
  mat = matrix(rep(0,26*n),nrow=26, ncol=n);
  for (i in 1:n)
  {
    c = str2int(ens[i]);
    mat[c,i] <- 1;
  }
  
  #creation d'une liste de taille 26 avec le nombre de mot contenant chaque lettre
  h <- vector(length=26)
  for (i in 1:26)
  {
    h[i] = sum(mat[i,])
  }
  
  #probabilite qu'une lettre apparaisse dans un mot
  p <- h / n
  #inverse de la probabilite precedente
  pInv <- 1 - p
  
  #calcul de l entropie
  entropie <- -(log2(p ^ p) + log2(pInv ^ pInv))
  
  #position de la lettre la plus interessante a choisir
  position <- which.max(entropie)
  
  ensA <- c()
  ensS <- c()
  
  for (i in 1:n)
  {
    if(mat[position,i] == 1 ){
      ensA = c(ensA, ens[i])
    } else {
      ensS = c(ensS, ens[i])
    }
  }
  
  return(list(pos = position, ensembleA = ensA, ensembleS = ensS))
} 

jouer <- function(ens) {
  
  mot <- scan("",what="character",nlines=1);
  size <- nchar(mot)
  tmpEns <- ens
  
  while(length(tmpEns) != 1) {
    res <- getIndice(tmpEns)
    
     print(paste("lettre = ", int2st(res$pos)))
    
    if(containLetter(res$pos, mot) == TRUE) {
      tmpEns <- res$ensembleA
    } else {
      tmpEns <- res$ensembleS
    }
  }

  print(paste("mot = ", tmpEns))
}

containLetter <- function(letter, word) {
  
  size <- nchar(word)
  chaine <- unlist(strsplit(word, split=""))
  
  for(i in 1:size) {
    if(str2int(chaine[i]) == letter) {
      return(TRUE);
    }
  }
  return(FALSE);
}

calcArbre <- function(ens, cpt) {
  
  if(length(ens) <= 1)
    return ()
  
  attribut <- getIndice(ens)
  print(paste(cpt, int2st(attribut$pos)))
  
  calcArbre(attribut$ensembleA, cpt+1)
  calcArbre(attribut$ensembleS, cpt+1)
  
}
  \end{lstlisting}

\end{document}  