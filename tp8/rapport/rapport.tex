% --------------------------------------
% Document Class
% --------------------------------------
\documentclass[a4paper,11pt]{article}
% --------------------------------------



% --------------------------------------
% Use Package
% --------------------------------------


\usepackage[francais]{babel}
%\usepackage{ucs}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{makeidx}
\usepackage{color}
\usepackage{graphicx}
\usepackage{float}
\usepackage[hidelinks]{hyperref} 
\usepackage{geometry}
%\usepackage{lastpage}
%\usepackage{marginnote}
\usepackage{fancyhdr}
%\usepackage{titlesec}
%\usepackage{framed}
\usepackage{amsmath}
\usepackage{empheq}
\usepackage{array}
\usepackage{multicol}
%\usepackage{adjustbox}

% insert code
\usepackage{listings}

% define our color
\usepackage{xcolor}

% code color
\definecolor{ligthyellow}{RGB}{250,247,220}
\definecolor{darkblue}{RGB}{5,10,85}
\definecolor{ligthblue}{RGB}{1,147,128}
\definecolor{darkgreen}{RGB}{8,120,51}
\definecolor{darkred}{RGB}{160,0,0}

% other color
\definecolor{ivi}{RGB}{141,107,185}


\lstset{
  language=Scilab,
  captionpos=b,
  extendedchars=true,
  frame=lines,
  numbers=left,
  numberstyle=\tiny,
  numbersep=5pt,
  keepspaces=true,
  breaklines=true,
  showspaces=false,
  showstringspaces=false,
  breakatwhitespace=false,
  stepnumber=1,
  showtabs=false,
  tabsize=3,
  basicstyle=\small\ttfamily,
  backgroundcolor=\color{ligthyellow},
  keywordstyle=\color{ligthblue},
  morekeywords={include, printf, uchar},
  identifierstyle=\color{darkblue},
  commentstyle=\color{darkgreen},
  stringstyle=\color{darkred},
}


% --------------------------------------



% --------------------------------------
% Page setting
% --------------------------------------
%\pagestyle{empty}
\setlength{\headheight}{15pt}

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{2}

\makeatletter
\@addtoreset{chapter}{part}
\makeatother 

\hypersetup{         % parametrage des hyperliens
  colorlinks=true,      % colorise les liens
  breaklinks=true,      % permet les retours à la ligne pour les liens trop longs
  urlcolor= blue,       % couleur des hyperliens
  linkcolor= black,     % couleur des liens internes aux documents (index, figures, tableaux, equations,...)
  citecolor= green      % couleur des liens vers les references bibliographiques
}

% --------------------------------------

% --------------------------------------
% Information
% --------------------------------------
\title{Compte-rendu TP8 Rdf : Classifcation non supervisée}
\author{Elliot VANEGUE et Gaëtan DEFLANDRE}
% --------------------------------------

\definecolor{myColor}{rgb}{0.5, 0.1, 0.75}

% --------------------------------------
% Begin content
% --------------------------------------
\begin{document}
  
  % Set language to english
  \selectlanguage{francais}
  
  % Start the page counting
  \pagenumbering{arabic}
  
  \maketitle
  
  \mbox{}
  \newpage
  \clearpage
  
  \section*{Introduction}
  Jusque là nous avons vu des méthodes de classification supervisée, c'est à dire qu'elles produisaient
  des règles automatiquement à partir de données fournies au préalable. 
  Lors de ce TP, nous allons voir une méthode non supervisée qui est la méthode du K-means. Cette méthode
  va permettre de séparer les données de l'image en plusieurs groupes sans fournir de base de données. Nous allons
  nous servir de cette technique afin de trouver le seuil le plus approprié à la binarisation d'une image.
  
  \section{Classification des données Iris par la méthode K-means}
  Dans un premier temps, nous classifions les données Iris en trois classes représentant les trois espèces
  d'iris. Pour différencier les iris, nous avons quatre caractéristiques fournies dans les données : la largeur et
  la longueur du sépale et la largeur et la hauteur du pétale.\\
  
  \begin{figure}[H]
    \center
    \includegraphics[width=9cm]{resultat/separation_espece.png}
    \caption{Graphique de séparation des données des trois espèces d'iris}
  \end{figure}
  
  Nous appliquons la classification K-means sur ces données afin de voir si celui-ci sépare correctement les données.
  Le principe de l'algorithme du K-means est de minimiser la distance entre le centre d'une classe et les données
  qui la constituent. Pour cela, ces centres sont le plus éloigné entre eux à la première itération, puis ils se 
  déplacent à chaque itération jusqu'à se retrouver au centre des données d'une classe.\\
  
  %TODO réfléchir à pourquoi on a des données qui ont deux formes
  Lorsque nous effectuons cet algorithme sur les données Iris avec quinze itérations, nous pouvons voir que l'un des centres se trouve 
  entre deux classes, tandis que les deux autres se retrouvent sur la même classe.
  
  \begin{figure}[H]
    \center
    \includegraphics[width=9cm]{resultat/kmeans.png}
    \caption{Résultat de la classification des données Iris par K-means}
  \end{figure}
  
  Afin de détecter d'où viennent les erreurs, nous avons répété le processus précédent cinq fois en y ajoutant
  une variable permettant de calculer les erreurs. Ainsi nous voyons que le centre se déplace et atteint une
  position permettant de bien séparer chaque classe. Voici un tableau comportant la position de chaque centre
  sur les cinq itérations, ainsi que le taux d'erreurs de la classification.\\
  
  \begin{center}
  \begin{tabular}{|c|c|c|c|c|}
    \hline
    iteration & centre 1 & centre 2 & centre 3 & taux d'erreur\\
    \hline
    1 & (3.62 , 0.27) & (2.90 , 0.35) & (2.89 , 1.70) & 0.33\\
    \hline
    2 & (2.74 , 1.43) & (3.42 , 0.24) & (3.07 , 2.07) & 0.10\\
    \hline
    3 & (3.42 , 0.24) & (2.74 , 1.43) & (3.07 , 2.07) & 0.10\\
    \hline
    4 & (2.90 , 0.35) & (2.89 , 1.70) & (3.62 , 0.27) & 0.33\\
    \hline
    5 & (3.07 , 2.07) & (2.74 , 1.43) & (3.42 , 0.24) & 0.10\\
    \hline
  \end{tabular}
  \end{center}
  
  Nous pouvons voir avec ces résultats que même si le taux d'erreur est minime cela ne signifie pas
  que les points sont parfaitement au centre des classes de données. A chaque itération, les points
  sont en mouvement pour atteindre un stade final avec un taux d'erreur minimum et un bon positionnement.
  %la pas sur mais je vois pas quoi en conclure.
%   On ne remarque pas de gros changements entre les itérations, à part pour le point 1 entre les itérations
%   4 et 5. De plus, le taux d'erreurs varie, mais il revient sur d'anciennes valeurs, ce qui signifie que les 
%   cinq itérations que nous venons de réaliser ne servent pas et que les centres étaient proche de leur 
%   état final.
  \begin{figure}[H]
    \center
    \includegraphics[width=9cm]{resultat/5_ite.png}
    \caption{Graphique avec les centres des classes après cinq itération du K-means}
  \end{figure}
  
  %TODO faire la suite
  
  \section{Segmentation d'une image de textures par classification non supervisée des pixels}
  Maintenant nous allons segmenter l'image des cercles étudiée lors du TP3 grâce à la classification
  K-means. Nous disposons des images d'intensités de gris et de textures.
  
  \begin{figure}[H]
    \center
    \includegraphics[width=3cm]{resultat/rdf-2-classes-texture-1.png}
    \includegraphics[width=3cm]{resultat/rdf-2-classes-texture-1-text.png}
    \caption{À gauche l'image des intensités de gris. À droite l'image de texture.}
  \end{figure}
  
  Nous créons une matrice composée des données des deux images. Nous pouvons voir que le nuage
  de points est composé de deux formes distinctes : l'une est à l'horizontale et relativement grosse, tandis 
  que l'autre est parfaitement à la verticale.
  
  \begin{figure}[H]
    \center
    \includegraphics[width=9cm]{resultat/image_combine.png}
    \caption{Graphique des données des deux images}
  \end{figure}
  
  Nous effectuons la classification des données précédentes avec K-means en 30 itérations. Le résultat
  nous donne une classification diagonale avec comme premier centre, le centre de la forme horizontale et 
  comme second centre le bas de la forme verticale.\\
  
  Nous regroupons ensuite les points en fonction de la distance avec les deux centres trouvées. 
  Un premier groupe de points plus proches du premier centre et un second groupe de points plus 
  proches du second centre.\\
  
  Nous constatons que cette méthode n'est pas optimale, car la classe verticale est légèrement coupé.\\
  
  \begin{figure}[H]
    \center
    \includegraphics[width=9cm]{resultat/classification_gateau.png}
    \caption{Résultat de la classification de K-means des données des deux images}
  \end{figure}
  
  Pour finir, nous reconstruisons l'image binaire correspondante\ldots
  
  \begin{figure}[H]
    \center
    \includegraphics[width=3cm]{resultat/segmentation_cercle.png}
    \caption{Segmentation en fonction de l'image des intensités de gris et de l'image de texture.}
  \end{figure}
  
\end{document}  